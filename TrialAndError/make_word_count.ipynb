{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from NLP_utils import *\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 파일 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기존 파일 이름과 확장자\n",
    "input_file_name = \"all_texts.txt\"\n",
    "\n",
    "# 분할된 파일들이 저장될 디렉토리 생성\n",
    "output_directory = \"split_files\"\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# 기존 파일을 읽어오고 줄 단위로 처리\n",
    "with open(input_file_name, \"r\", encoding=\"utf-8\") as input_file:\n",
    "    lines = [line.strip() for line in input_file.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45579550"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# 분할된 파일들이 저장될 디렉토리 생성\n",
    "output_directory = \"split_files\"\n",
    "\n",
    "# 분할할 개수 계산\n",
    "total_lines = 45579550      # 총 line 수\n",
    "lines_per_file = total_lines // 100\n",
    "remainder = total_lines % 100\n",
    "\n",
    "# 100개의 작은 파일로 분할\n",
    "start = 0\n",
    "for i in tqdm(range(100), desc=\"File split\"):\n",
    "    file_number = i + 1\n",
    "    end = start + lines_per_file + (1 if i < remainder else 0)\n",
    "    output_file_name = os.path.join(output_directory, f\"sentence_split_{file_number}.txt\")\n",
    "    splitted = lines[start:end]\n",
    "    with open(output_file_name, \"w\", encoding=\"utf-8\") as output_file:\n",
    "        for line in splitted:\n",
    "            output_file.write(line + \"\\n\")\n",
    "    start = end\n",
    "\n",
    "print(\"파일 분할이 완료되었습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 형태소 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from NLP_utils import *\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import google.protobuf.text_format as tf\n",
    "from bareunpy import Tagger\n",
    "\n",
    "API_KEY=\"koba-BRBCJ3A-OA4UPJY-QXV5GSI-MU2WLMY\"  # 본인 API 키 등록\n",
    "tagger = Tagger(API_KEY, 'localhost')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[\"NNG\", \"NNP\", \"VV\", \"VA\", \"MAG\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------- 1th / 100th work start -------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 119/455796 [00:05<5:20:33, 23.69it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m morpheme_list \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m tqdm(sentence_list):\n\u001b[1;32m---> 12\u001b[0m     pos_text \u001b[38;5;241m=\u001b[39m tagger\u001b[38;5;241m.\u001b[39mpos(sentence)\n\u001b[0;32m     13\u001b[0m     morpheme_list\u001b[38;5;241m.\u001b[39mappend(pos_text)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# 이중 리스트 -> 단일 리스트\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Junho\\Anaconda3\\Lib\\site-packages\\bareunpy\\_tagger.py:260\u001b[0m, in \u001b[0;36mTagger.pos\u001b[1;34m(self, phrase, flatten, join, detail)\u001b[0m\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpos\u001b[39m(\u001b[38;5;28mself\u001b[39m, phrase: \u001b[38;5;28mstr\u001b[39m, flatten: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, join: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, detail: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List:\n\u001b[0;32m    253\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    254\u001b[0m \u001b[38;5;124;03m    POS tagger.\u001b[39;00m\n\u001b[0;32m    255\u001b[0m \u001b[38;5;124;03m    :param phrase  : string to analyse\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;124;03m    :param detail  : if True, returns every things of morph result\u001b[39;00m\n\u001b[0;32m    259\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 260\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtag(phrase)\u001b[38;5;241m.\u001b[39mpos(flatten, join, detail)\n",
      "File \u001b[1;32mc:\\Users\\Junho\\Anaconda3\\Lib\\site-packages\\bareunpy\\_tagger.py:210\u001b[0m, in \u001b[0;36mTagger.tag\u001b[1;34m(self, phrase, auto_split, auto_spacing, auto_jointing)\u001b[0m\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Tagged(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, AnalyzeSyntaxResponse())\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 210\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39manalyze_syntax(phrase, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdomain, auto_split\u001b[38;5;241m=\u001b[39mauto_split, auto_spacing\u001b[38;5;241m=\u001b[39mauto_spacing, auto_jointing\u001b[38;5;241m=\u001b[39mauto_jointing)\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Tagged(phrase, res)\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\Junho\\Anaconda3\\Lib\\site-packages\\bareunpy\\_lang_service_client.py:61\u001b[0m, in \u001b[0;36mBareunLanguageServiceClient.analyze_syntax\u001b[1;34m(self, content, domain, auto_split, auto_spacing, auto_jointing)\u001b[0m\n\u001b[0;32m     59\u001b[0m     req\u001b[38;5;241m.\u001b[39mcustom_domain \u001b[38;5;241m=\u001b[39m domain\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 61\u001b[0m     res, c \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstub\u001b[38;5;241m.\u001b[39mAnalyzeSyntax\u001b[38;5;241m.\u001b[39mwith_call(\n\u001b[0;32m     62\u001b[0m         request\u001b[38;5;241m=\u001b[39mreq, metadata\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata)\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\Junho\\Anaconda3\\Lib\\site-packages\\grpc\\_channel.py:1175\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable.with_call\u001b[1;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[0;32m   1163\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwith_call\u001b[39m(\n\u001b[0;32m   1164\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1165\u001b[0m     request: Any,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1170\u001b[0m     compression: Optional[grpc\u001b[38;5;241m.\u001b[39mCompression] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1171\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Any, grpc\u001b[38;5;241m.\u001b[39mCall]:\n\u001b[0;32m   1172\u001b[0m     (\n\u001b[0;32m   1173\u001b[0m         state,\n\u001b[0;32m   1174\u001b[0m         call,\n\u001b[1;32m-> 1175\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_blocking(\n\u001b[0;32m   1176\u001b[0m         request, timeout, metadata, credentials, wait_for_ready, compression\n\u001b[0;32m   1177\u001b[0m     )\n\u001b[0;32m   1178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _end_unary_response_blocking(state, call, \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\Junho\\Anaconda3\\Lib\\site-packages\\grpc\\_channel.py:1142\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable._blocking\u001b[1;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[0;32m   1126\u001b[0m state\u001b[38;5;241m.\u001b[39mmethod \u001b[38;5;241m=\u001b[39m _common\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_method)\n\u001b[0;32m   1127\u001b[0m call \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_channel\u001b[38;5;241m.\u001b[39msegregated_call(\n\u001b[0;32m   1128\u001b[0m     cygrpc\u001b[38;5;241m.\u001b[39mPropagationConstants\u001b[38;5;241m.\u001b[39mGRPC_PROPAGATE_DEFAULTS,\n\u001b[0;32m   1129\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_method,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1140\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_context,\n\u001b[0;32m   1141\u001b[0m )\n\u001b[1;32m-> 1142\u001b[0m event \u001b[38;5;241m=\u001b[39m call\u001b[38;5;241m.\u001b[39mnext_event()\n\u001b[0;32m   1143\u001b[0m _handle_event(event, state, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_deserializer)\n\u001b[0;32m   1144\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m state, call\n",
      "File \u001b[1;32msrc\\python\\grpcio\\grpc\\_cython\\_cygrpc/channel.pyx.pxi:366\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc.SegregatedCall.next_event\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32msrc\\python\\grpcio\\grpc\\_cython\\_cygrpc/channel.pyx.pxi:189\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._next_call_event\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Junho\\Anaconda3\\Lib\\threading.py:264\u001b[0m, in \u001b[0;36mCondition.__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39m_at_fork_reinit()\n\u001b[0;32m    262\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_waiters\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m--> 264\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__enter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    265\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__enter__\u001b[39m()\n\u001b[0;32m    267\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "output_directory = \"split_morphemes\"\n",
    "\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "for i in range(1, 101):\n",
    "    print(f\"------------------------- {i}th / 100th work start -------------------------\")\n",
    "    print()\n",
    "    sentence_list = txt_to_list(f\"split_files/sentence_split_{i}.txt\")\n",
    "\n",
    "    morpheme_list = []\n",
    "    for sentence in tqdm(sentence_list):\n",
    "        pos_text = tagger.pos(sentence)\n",
    "        morpheme_list.append(pos_text)\n",
    "    \n",
    "    # 이중 리스트 -> 단일 리스트\n",
    "    classic_sentences = double_to_single(morpheme_list)\n",
    "    print(len(classic_sentences))\n",
    "\n",
    "    with open(f\"split_morphemes/morpheme_split_{i}.txt\", \"w\") as f:\n",
    "        for i in classic_sentences:\n",
    "            f.write(i + \"\\n\")\n",
    "    print()\n",
    "    print(f\"------------------------- {i}th / 100th work Done -------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
